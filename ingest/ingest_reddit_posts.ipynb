{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f5b48eb-05e5-4300-b930-3dcebe1642fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b363ac52-8ae5-4157-8bce-335a25bc4792",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import datetime as dt\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Third-party library imports\n",
    "import asyncpraw\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "from pyspark.sql.types import (\n",
    "    DateType,\n",
    "    BooleanType,\n",
    "    FloatType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    TimestampType\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "reddit_api_client_id = os.getenv('REDDIT_API_CLIENT_ID')\n",
    "reddit_api_client_secret = os.getenv('REDDIT_API_CLIENT_SECRET')\n",
    "reddit_api_user_agent = os.getenv('REDDIT_API_USER_AGENT')\n",
    "catalog_name = os.getenv('DATABRICKS_CATALOG_NAME')\n",
    "schema_name = os.getenv('DATABRICKS_SCHEMA_NAME')\n",
    "\n",
    "reddit = asyncpraw.Reddit(\n",
    "    client_id=reddit_api_client_id,\n",
    "    client_secret=reddit_api_client_secret,\n",
    "    user_agent=reddit_api_user_agent,\n",
    "    ratelimit_seconds=600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b43e5050-c494-4f62-a73c-12aa3e32495a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reddit_post_schema = StructType([\n",
    "    StructField(\"ticker_symbol\", StringType(), True),\n",
    "    StructField(\"post_id\", StringType(), True),\n",
    "    StructField(\"post_title\", StringType(), True),\n",
    "    StructField(\"subreddit_id\", StringType(), True),\n",
    "    StructField(\"subreddit\", StringType(), True),\n",
    "    StructField(\"created_utc\", DateType(), True),\n",
    "    StructField(\"score\", IntegerType(), True),\n",
    "    StructField(\"upvote_ratio\", FloatType(), True),\n",
    "    StructField(\"num_comments\", IntegerType(), True),\n",
    "    StructField(\"post_body_text\", StringType(), True),\n",
    "    StructField(\"is_self_post\", BooleanType(), True),\n",
    "    StructField(\"is_original_content\", BooleanType(), True),\n",
    "    StructField(\"permalink\", StringType(), True),\n",
    "    StructField(\"post_url\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23d2d0c0-bbd7-4142-a087-c45ccfd079d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Get S&P 500 Company Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0691d665-cb5d-49bb-8936-dff3e800218d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SP500_tickers = (spark.read.table(f'{catalog_name}.{schema_name}.kdayno_bronze_SP500_companies')\n",
    "                  .select('ticker_symbol')\n",
    "                )\n",
    "\n",
    "SP500_tickers_list = [row['ticker_symbol'] for row in SP500_tickers.collect()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e3cf72e-543f-418b-940a-8cc415208b9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load \"Top\" Posts data for multiple Subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9b3fbf3-c571-489e-8ce9-f5a578924bc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "subreddits = ['stocks', 'investing', 'trading', 'wallstreetbets']\n",
    "keywords = SP500_tickers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a9bf5c2-d689-47e8-9fdf-ba277d0b7961",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"DELETE FROM {catalog_name}.{schema_name}.kdayno_bronze_reddit_top_posts\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08cbe492-3e4e-40d4-b706-fa705576349d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for subreddit in subreddits:\n",
    "\n",
    "    sr = await reddit.subreddit(f\"{subreddit}\")\n",
    "\n",
    "    post_data = {'ticker_symbol':[], 'post_id':[], 'post_title':[], 'subreddit_id':[], 'subreddit':[],  \n",
    "                 'created_utc':[], 'score':[], 'upvote_ratio':[],'num_comments':[], 'post_body_text':[], \n",
    "                 'is_self_post':[], 'is_original_content':[], 'permalink':[], 'post_url':[] }\n",
    "\n",
    "    for keyword in keywords:  \n",
    "\n",
    "        print(f'Getting data for ticker: {keyword} ...')\n",
    "\n",
    "        posts = sr.search(keyword, sort='top', time_filter=\"year\", limit=500)\n",
    "\n",
    "        async for post in posts:\n",
    "\n",
    "            print(f'Getting data for post: {post.title} ...')\n",
    "\n",
    "            post_data['ticker_symbol'].append(str(keyword))\n",
    "            post_data['post_id'].append(post.id)\n",
    "            post_data['post_title'].append(post.title) \n",
    "            post_data['subreddit_id'].append(post.subreddit_id)\n",
    "            post_data['subreddit'].append(str(post.subreddit))\n",
    "            post_data['created_utc'].append(dt.datetime.fromtimestamp(post.created_utc))\n",
    "            post_data['score'].append(post.score)\n",
    "            post_data['upvote_ratio'].append(post.upvote_ratio)\n",
    "            post_data['num_comments'].append(post.num_comments)\n",
    "            post_data['post_body_text'].append(post.selftext)\n",
    "            post_data['is_self_post'].append(post.is_self)\n",
    "            post_data['is_original_content'].append(post.is_original_content)\n",
    "            post_data['permalink'].append(post.permalink)\n",
    "            post_data['post_url'].append(post.url)\n",
    "\n",
    "    # Unpacks the dict values, creates tuples, then converts to a list of tuples, where each tuple contains the data for a given Reddit post\n",
    "    reddit_top_posts_df = spark.createDataFrame(list(zip(*post_data.values())), reddit_post_schema)\n",
    "\n",
    "    reddit_top_posts_df = reddit_top_posts_df.withColumn('load_date_ts', current_timestamp())\n",
    "\n",
    "    reddit_top_posts_df.write.format(\"delta\").mode(\"append\").partitionBy('subreddit').saveAsTable(f'{catalog_name}.{schema_name}.kdayno_bronze_reddit_top_posts')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f579dd81-533f-4eaa-b154-d4757d398178",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load \"Hot\" Posts data for multiple Subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7563e330-35da-4e2a-bcb7-d8e0592a99f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "time.sleep(60) # Stagger the next loop to avoid hitting the rate limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6597f258-72b5-4dcc-a559-9a49cbe6e4cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"DELETE FROM {catalog_name}.{schema_name}.kdayno_bronze_reddit_hot_posts\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba116925-3b98-4294-be1d-f76144dc06b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for subreddit in subreddits:\n",
    "\n",
    "    sr = await reddit.subreddit(f\"{subreddit}\")\n",
    "\n",
    "    post_data = {'ticker_symbol':[], 'post_id':[], 'post_title':[], 'subreddit_id':[], 'subreddit':[],  \n",
    "                 'created_utc':[], 'score':[], 'upvote_ratio':[],'num_comments':[], 'post_body_text':[], \n",
    "                 'is_self_post':[], 'is_original_content':[], 'permalink':[], 'post_url':[] }\n",
    "\n",
    "    for keyword in keywords:  \n",
    "\n",
    "        print(f'Getting data for ticker: {keyword} ...')\n",
    "\n",
    "        posts = sr.search(keyword, sort='hot', time_filter=\"month\", limit=500)\n",
    "\n",
    "        async for post in posts:\n",
    "\n",
    "            print(f'Getting data for post: {post.title} ...')\n",
    "\n",
    "            post_data['ticker_symbol'].append(str(keyword))\n",
    "            post_data['post_id'].append(post.id)\n",
    "            post_data['post_title'].append(post.title) \n",
    "            post_data['subreddit_id'].append(post.subreddit_id)\n",
    "            post_data['subreddit'].append(str(post.subreddit))\n",
    "            post_data['created_utc'].append(dt.datetime.fromtimestamp(post.created_utc))\n",
    "            post_data['score'].append(post.score)\n",
    "            post_data['upvote_ratio'].append(post.upvote_ratio)\n",
    "            post_data['num_comments'].append(post.num_comments)\n",
    "            post_data['post_body_text'].append(post.selftext)\n",
    "            post_data['is_self_post'].append(post.is_self)\n",
    "            post_data['is_original_content'].append(post.is_original_content)\n",
    "            post_data['permalink'].append(post.permalink)\n",
    "            post_data['post_url'].append(post.url)\n",
    "\n",
    "    # Unpacks the dict values, creates tuples, then converts to a list of tuples, where each tuple contains the data for a given Reddit post\n",
    "    reddit_hot_posts_df = spark.createDataFrame(list(zip(*post_data.values())), reddit_post_schema)\n",
    "    \n",
    "    reddit_hot_posts_df = reddit_hot_posts_df.withColumn('load_date_ts', current_timestamp())\n",
    "\n",
    "    reddit_hot_posts_df.write.format(\"delta\").mode(\"append\").partitionBy('subreddit').saveAsTable(f'{catalog_name}.{schema_name}.kdayno_bronze_reddit_hot_posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "163f05ab-2df0-41a4-a6c1-76d616c8387e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6368848666126293,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ingest_reddit_posts",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
